{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0\n",
      "125.106\n",
      "step 100, training accuracy 0.2\n",
      "34.2457\n",
      "step 200, training accuracy 0.3\n",
      "32.9542\n",
      "step 300, training accuracy 0.3\n",
      "31.7501\n",
      "step 400, training accuracy 0.4\n",
      "27.5895\n",
      "step 500, training accuracy 0.8\n",
      "14.6458\n",
      "step 600, training accuracy 0.5\n",
      "21.5096\n",
      "step 700, training accuracy 0.9\n",
      "13.0551\n",
      "step 800, training accuracy 0.9\n",
      "9.16472\n",
      "step 900, training accuracy 1\n",
      "4.81043\n",
      "step 1000, training accuracy 1\n",
      "4.34846\n",
      "step 1100, training accuracy 1\n",
      "4.28719\n",
      "step 1200, training accuracy 1\n",
      "9.08192\n",
      "step 1300, training accuracy 0.9\n",
      "4.69425\n",
      "step 1400, training accuracy 1\n",
      "0.765753\n",
      "step 1500, training accuracy 1\n",
      "6.20816\n",
      "step 1600, training accuracy 1\n",
      "5.09926\n",
      "step 1700, training accuracy 1\n",
      "0.414577\n",
      "step 1800, training accuracy 1\n",
      "1.85306\n",
      "step 1900, training accuracy 0.9\n",
      "5.9861\n",
      "step 2000, training accuracy 1\n",
      "0.218542\n",
      "step 2100, training accuracy 1\n",
      "1.67132\n",
      "step 2200, training accuracy 1\n",
      "0.585665\n",
      "step 2300, training accuracy 1\n",
      "0.173789\n",
      "step 2400, training accuracy 0.9\n",
      "2.48017\n",
      "step 2500, training accuracy 1\n",
      "1.94091\n",
      "step 2600, training accuracy 1\n",
      "0.800524\n",
      "step 2700, training accuracy 1\n",
      "0.16504\n",
      "step 2800, training accuracy 1\n",
      "1.05461\n",
      "step 2900, training accuracy 0.9\n",
      "3.5514\n",
      "step 3000, training accuracy 1\n",
      "0.84836\n",
      "step 3100, training accuracy 1\n",
      "0.170457\n",
      "step 3200, training accuracy 1\n",
      "1.12881\n",
      "step 3300, training accuracy 1\n",
      "0.169389\n",
      "step 3400, training accuracy 1\n",
      "0.0145745\n",
      "step 3500, training accuracy 1\n",
      "0.0590604\n",
      "step 3600, training accuracy 0.9\n",
      "7.89856\n",
      "step 3700, training accuracy 1\n",
      "0.0642404\n",
      "step 3800, training accuracy 1\n",
      "0.00671233\n",
      "step 3900, training accuracy 1\n",
      "0.13188\n",
      "step 4000, training accuracy 1\n",
      "0.231647\n",
      "step 4100, training accuracy 1\n",
      "0.409404\n",
      "step 4200, training accuracy 1\n",
      "1.48765\n",
      "step 4300, training accuracy 1\n",
      "0.889796\n",
      "step 4400, training accuracy 1\n",
      "0.861571\n",
      "step 4500, training accuracy 1\n",
      "0.899436\n",
      "step 4600, training accuracy 1\n",
      "2.62169\n",
      "step 4700, training accuracy 1\n",
      "0.435331\n",
      "step 4800, training accuracy 1\n",
      "0.106088\n",
      "step 4900, training accuracy 1\n",
      "0.00150599\n",
      "validation accuracy 1\n",
      "标签3的召回率是：0.903\n",
      "标签31的召回率是：0.996\n",
      "标签7的召回率是：0.873\n",
      "标签2的召回率是：0.900\n",
      "标签19的召回率是：0.901\n",
      "标签5的召回率是：0.903\n",
      "标签27的召回率是：0.877\n",
      "标签10的召回率是：0.959\n",
      "标签32的召回率是：0.933\n",
      "标签9的召回率是：0.919\n",
      "标签12的召回率是：0.907\n",
      "标签1的召回率是：0.914\n",
      "标签8的召回率是：0.908\n",
      "标签17的召回率是：0.934\n",
      "标签21的召回率是：0.921\n",
      "标签15的召回率是：0.858\n",
      "标签6的召回率是：0.901\n",
      "标签13的召回率是：0.805\n",
      "标签20的召回率是：0.890\n",
      "标签0的召回率是：0.903\n",
      "标签14的召回率是：0.972\n",
      "标签33的召回率是：0.870\n",
      "标签24的召回率是：0.850\n",
      "标签30的召回率是：0.896\n",
      "标签11的召回率是：0.903\n",
      "标签23的召回率是：0.880\n",
      "标签4的召回率是：0.872\n",
      "标签29的召回率是：0.889\n",
      "标签16的召回率是：0.892\n",
      "标签22的召回率是：0.877\n",
      "标签18的召回率是：0.926\n",
      "标签28的召回率是：0.895\n",
      "标签25的召回率是：0.901\n",
      "标签26的召回率是：0.901\n",
      "running time is: 1079.903654\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "\n",
    "#开始时间\n",
    "start = time.clock()\n",
    "\n",
    "\n",
    "\n",
    "#导入训练集\n",
    "fileNameQue = tf.train.string_input_producer([\"tfrecords/train.tfrecords\"])\n",
    "reader = tf.TFRecordReader()\n",
    "key,value = reader.read(fileNameQue)\n",
    "features = tf.parse_single_example(value,features={ 'label': tf.FixedLenFeature([], tf.int64),\n",
    "                                           'img_raw' : tf.FixedLenFeature([], tf.string)})\n",
    "\n",
    "image = tf.decode_raw(features[\"img_raw\"], tf.uint8)\n",
    "#归一化\n",
    "image=tf.cast(image,tf.float32)*(1/255)\n",
    "image=tf.reshape(image,[48,24,3])\n",
    "#图片转变为黑白图片\n",
    "image=tf.split(image,3,2)[0]\n",
    "\n",
    "label=features['label']\n",
    "label = tf.cast(label, tf.int32)\n",
    "init = tf.global_variables_initializer()\n",
    "# 随机选取图片形成一个ｂａｔｃｈ\n",
    "imageBatch, labelBatch = tf.train.shuffle_batch([image, label], batch_size = 10,capacity = 20000, min_after_dequeue = 19000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#导入测试集\n",
    "TfileNameQue = tf.train.string_input_producer([\"tfrecords/validation.tfrecords\"])\n",
    "Treader = tf.TFRecordReader()\n",
    "Tkey,Tvalue = Treader.read(TfileNameQue)\n",
    "Tfeatures = tf.parse_single_example(Tvalue,features={ 'label': tf.FixedLenFeature([], tf.int64),\n",
    "                                           'img_raw' : tf.FixedLenFeature([], tf.string)})\n",
    "\n",
    "Timage = tf.decode_raw(Tfeatures[\"img_raw\"], tf.uint8)\n",
    "Timage=tf.cast(Timage,tf.float32)*(1/255)\n",
    "Timage=tf.reshape(Timage,[48,24,3])\n",
    "Timage=tf.split(Timage,3,2)[0]\n",
    "\n",
    "Tlabel=Tfeatures['label']\n",
    "Tlabel = tf.cast(Tlabel, tf.int32)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "TimageBatch, TlabelBatch = tf.train.shuffle_batch([Timage, Tlabel], batch_size = 10,capacity = 3000, min_after_dequeue = 2900)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 一，函数声明部分\n",
    "\n",
    "#权重\n",
    "def weight_variable(shape):\n",
    "    # 正态分布，标准差为0.1，默认最大为1，最小为-1，均值为0\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#偏差\n",
    "def bias_variable(shape):\n",
    "    # 创建一个结构为shape矩阵也可以说是数组shape声明其行列，初始化所有值为0.1\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#卷积\n",
    "def conv2d(x, W):  \n",
    "    # 卷积遍历各方向步数为1，SAME：边缘外自动补0，遍历相乘\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')  \n",
    "\n",
    "#最大池化\n",
    "def max_pool_3x3(x):  \n",
    "    # 池化卷积结果（conv2d）池化层采用kernel大小为2*2，步数也为2，周围补0，取最大值。数据量缩小了4倍\n",
    "    return tf.nn.max_pool(x, ksize=[1, 3, 3, 1],strides=[1, 3, 3, 1], padding='SAME')  \n",
    "\n",
    "# 二，定义输入输出结构\n",
    "\n",
    "# 声明一个占位符，None表示输入图片的数量不定，28*28图片分辨率\n",
    "xs = tf.placeholder(tf.float32, [None, 48*24]) \n",
    "# 类别是0-9总共10个类别，对应输出分类结果\n",
    "ys = tf.placeholder(tf.int32, [None]) \n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# x_image又把xs reshape成了48*24*1的形状，因为是灰色图片，所以通道是1.作为训练时的input，-1代表图片数量不定\n",
    "x_image = tf.reshape(xs, [-1, 48, 24, 1]) \n",
    "\n",
    "\n",
    "# 三，搭建网络,定义算法公式，也就是forward时的计算\n",
    "\n",
    "## 第一层卷积操作 ##\n",
    "# 第一二参数值得卷积核尺寸大小，即patch，第三个参数是图像通道数，第四个参数是卷积核的数目，代表会出现多少个卷积特征图像;\n",
    "W_conv1 = weight_variable([5, 5, 1, 32]) \n",
    "# 对于每一个卷积核都有一个对应的偏置量。\n",
    "b_conv1 = bias_variable([32])  \n",
    "# 图片乘以卷积核，并加上偏执量，卷积结果48x24x32\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)  \n",
    "# 池化结果16x8x32 卷积结果乘以池化卷积核\n",
    "h_pool1 = max_pool_3x3(h_conv1) \n",
    "\n",
    "## 第二层卷积操作 ##   \n",
    "# 32通道卷积，卷积出96个特征  \n",
    "w_conv2 = weight_variable([5,5,32,96]) \n",
    "# 96个偏执数据\n",
    "b_conv2  = bias_variable([96]) \n",
    "# 注意h_pool1是上一层的池化结果，#卷积结果16x8x96\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,w_conv2)+b_conv2)  \n",
    "\n",
    "h_pool2 = max_pool_3x3(h_conv2)  \n",
    "# 原图像尺寸48*24，第一轮图像缩小为16*8，共有32张，第二轮后图像缩小为6*3，共有96张  \n",
    "# 池化结果6x3x96\n",
    "\n",
    "## 第三层全连接操作 ##\n",
    "# 二维张量，第一个参数6*3*96的patch，也可以认为是只有一行6*3*96个数据的卷积，第二个参数代表卷积个数共1024个\n",
    "W_fc1 = weight_variable([6*3*96, 1024]) \n",
    "# 1024个偏执数据\n",
    "b_fc1 = bias_variable([1024]) \n",
    "# 将第二层卷积池化结果reshape成只有一行6*3*96个数据# [n_samples, 6, 3, 96] ->> [n_samples, 6*3*96]\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 6*3*96]) \n",
    "# 卷积操作，结果是1*1*1024，单行乘以单列等于1*1矩阵，matmul实现最基本的矩阵相乘，不同于tf.nn.conv2d的遍历相乘，自动认为是前行向量后列向量\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) \n",
    "\n",
    "# dropout操作，减少过拟合，其实就是降低上一层某些输入的权重scale，甚至置为0，升高某些输入的权值，甚至置为2，防止评测曲线出现震荡，个人觉得样本较少时很必要\n",
    "# 使用占位符，由dropout自动确定scale，也可以自定义，比如0.5，根据tensorflow文档可知，程序中真实使用的值为1/0.5=2，也就是某些输入乘以2，同时某些输入乘以0\n",
    "# keep_prob = tf.placeholder(tf.float32) \n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob) #对卷积结果执行dropout操作\n",
    "\n",
    "## 第四层输出操作 ##\n",
    "# 二维张量，1*1024矩阵卷积，共10个卷积，对应我们开始的ys长度为10\n",
    "W_fc2 = weight_variable([1024, 34])  \n",
    "b_fc2 = bias_variable([34])  \n",
    "# 最后的分类，结果为1*1*10 softmax和sigmoid都是基于logistic分类算法，一个是多分类一个是二分类\n",
    "y_conv=tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "# print(y_conv.shape)\n",
    "# 四，定义loss(最小误差概率)，选定优化优化loss，\n",
    "\n",
    "cross_entropy=tf.reduce_sum(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_conv,labels=ys))\n",
    "# cross_entropy=tf.reduce_sum(\n",
    "#     tf.nn.softmax_cross_entropy_with_logits(logits=y_conv,labels=ys))\n",
    "\n",
    "\n",
    "# cross_entropy=tf.reduce_sum(tf.argmax(y_conv,1),ys)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(cross_entropy) # 调用优化器优化，其实就是通过喂数据争取cross_entropy最小化  \n",
    "# print(train_step)\n",
    "# # 五，开始数据训练以及评测\n",
    "correct_prediction = tf.equal(tf.cast(tf.argmax(y_conv,1),tf.int32), ys)\n",
    "# print(correct_prediction)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "#模型保存\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# print(accuracy)\n",
    "import numpy as np\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    #队列\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    new_dict={}\n",
    "    \n",
    "    for i in range(5000):\n",
    "        img,lab=sess.run([imageBatch, labelBatch])\n",
    "        img=np.reshape(img,[-1,1152])\n",
    "        lab=np.reshape(lab,[-1])\n",
    "#         print(img.shape)\n",
    "#         print(lab.shape)\n",
    "        \n",
    "       \n",
    "        if i % 100 == 0:\n",
    "#            \n",
    "            train_accuracy = sess.run(accuracy,\n",
    "                                feed_dict={xs:img, ys: lab, keep_prob: 1.0})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            \n",
    "            saver.save(sess, 'save/sa.netmodel')\n",
    "        \n",
    "        \n",
    "        _, ce = sess.run([train_step, cross_entropy], feed_dict={xs:img, ys: lab, keep_prob: 0.5})\n",
    "        if i % 100 == 0:\n",
    "            print(ce)\n",
    "        \n",
    "        c, lab = sess.run([correct_prediction, ys], feed_dict={xs:img, ys:lab, keep_prob:1.0})\n",
    "\n",
    "        for j in range(len(c)):\n",
    "            if lab[j] in new_dict:\n",
    "                if c[j] == True:\n",
    "                    new_dict[lab[j]][0] += 1\n",
    "                    new_dict[lab[j]][1] += 1\n",
    "                else:\n",
    "                    new_dict[lab[j]][1] += 1\n",
    "            else:\n",
    "                if c[j] == True:\n",
    "                    new_dict[lab[j]] = [1, 1]\n",
    "                else:\n",
    "                    new_dict[lab[j]] = [0, 1]     \n",
    "                              \n",
    "                              \n",
    "    Timg,Tlab=sess.run([TimageBatch, TlabelBatch])\n",
    "    Timg=np.reshape(Timg,[-1,1152])\n",
    "    Tlab=np.reshape(Tlab,[-1])\n",
    "    \n",
    "    test_acc = accuracy.eval(feed_dict={xs:Timg, ys: Tlab, keep_prob: 1.0})\n",
    "    print(\"validation accuracy %g\" % test_acc)\n",
    "    \n",
    "    for k in new_dict:\n",
    "        print(\"标签%s的召回率是：%.3f\" %(k,  new_dict[k][0]/new_dict[k][1]))\n",
    "    \n",
    "    #结束时间\n",
    "    end = time.clock()\n",
    "    print('running time is:',end-start)\n",
    "    \n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
